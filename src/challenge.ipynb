{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Table\n",
    "The table **tweets** was created manually on BigQuery following this steps:\n",
    "1. Using the function **tweets_json_to_parquet** from the **utils** module to convert the json to a parquet file.\n",
    "2. Uploading manually the file to a Google Cloud Storage bucket.\n",
    "3. Create a table manually on the Google Cloud console by poiting to the previous parquet file so the table schema would be \"inherited\" from the parquet schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "from src.q1_time import q1_time\n",
    "from src.q2_memory import q2_memory\n",
    "from src.q2_time import q2_time\n",
    "from src.q3_memory import q3_memory\n",
    "from src.q3_time import q3_time\n",
    "from src.utils.gcp import download_file\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "from src.q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the variables used for each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/tmp/tweets_data.json'\n",
    "bucket_name = 'latam-de-challenge'\n",
    "source_blob = 'source-file/farmers-protest-tweets-2021-2-4.json'\n",
    "\n",
    "project_id = 'latam-de-428219'\n",
    "dataset = 'challenge_data'\n",
    "table = 'tweets_tb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks if the file is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(file_path):\n",
    "        download_file(bucket_name, source_blob, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines a function to print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result: Iterable) -> None:\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute functions bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q1_memory**\n",
    "\n",
    "This function executes the following steps: \n",
    "1. It reads the json file line by line extracting only the columns that are necessary for the code solution ('date', 'user').\n",
    "2. Converts the 'date' column to datetime64 and extracts only the date part.\n",
    "3. Applies a lambda function to the 'user' which is of struct type to extract the 'username' field.\n",
    "4. Counts the number of rows to amount of tweets grouped by day and user.\n",
    "5. Gets the row for each day where for the user that had the most tweets.\n",
    "6. Sorts the values by the total_tweets in descending order dropping duplicate dates and filtering the top 10.\n",
    "7. Iterates over the top 10 tweets and return the result.\n",
    "8. It catches exceptions for invalid column names, value errors and more broad exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q1_memory(file_path)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q2_memory**\n",
    "\n",
    "This function executes the following steps:\n",
    "1. Defines the regular expression for capturing emojis. \n",
    "2. It reads the json file line by line extracting only the columns that are necessary for the code solution ('content').\n",
    "3. Extracts the emojis from the 'content' column aggregating them in a string splitted by comma.\n",
    "4. Splits the string of emojis into a list using the comma as delimiter.\n",
    "5. Counts the number of each emoji creating 2 new columns in the dataframe ('emoji', 'emoji_count').\n",
    "6. Sorts the values by the emoji_count in descending order filtering the top 10 most used emojis.\n",
    "7. Iterates over the top 10 emojis and return the result.\n",
    "8. It catches exceptions for invalid column names, value errors and more broad exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q2_memory(file_path)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q3_memory**\n",
    "\n",
    "This function executes the following steps:\n",
    "1. Defines a private function **_get_mentioned_user** to extract the username from tweet mentions.\n",
    "2. It reads the json file line by line extracting only the columns that are necessary for the code solution ('mentionedUsers').\n",
    "3. Applies the function **_get_mentioned_user** to the column 'mentionedUsers'.\n",
    "4. Counts the number of each user creating 2 new columns in the dataframe ('user', 'mentions_count').\n",
    "5. Sorts the values by the mentions_count in descending order filtering the top 10 most used emojis.\n",
    "6. Iterates over the top 10 users and return the result.\n",
    "7. It catches exceptions for invalid column names, value errors and more broad exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q3_memory(file_path)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q1_time / q2_time / q3_time**\n",
    "\n",
    "The 3 functions for execution time follow the same steps describe bellow: \n",
    "1. Defines a query to be run on BigQuery\n",
    "2. Submits the request to BigQuery using the function **query_bigquery** from the utils module\n",
    "3. Iterates over the results from BigQuery and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q1_time(project_id, dataset, table)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q2_time(project_id, dataset, table)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q3_time(project_id, dataset, table)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "* For more and bigger files Apache Spark would have been a better approach to leverage massive and paralallel processing.\n",
    "* The JSON file could be converted to a Parquet file to leverage compression and the columnar file format for filter pruning.\n",
    "* More specific error handling can be implemented depending on the \"client's\" requirements.\n",
    "* Adding unit-testing functions with the pytest library\n",
    "* Use a python packaginng and dependency management like Poetry"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
